# ğŸ›ï¸ **Multimodal Product Retrieval with CLIP and Residual Adapters**

A **CLIP-based multimodal retrieval system** for e-commerce that aligns user text queries with product images in a shared embedding space.  
This project focuses on **domain adaptation without catastrophic misalignment**, achieving significant improvements over zero-shot CLIP while preserving its pre-trained semantic structure.

> ğŸ” Example query: *â€œa small orange plush toy with big eyesâ€*  
> ğŸ¯ Goal: retrieve visually accurate product images even when metadata is noisy or incomplete.

---

## ğŸ“Œ **Key Highlights**

- ğŸš« Identified **catastrophic misalignment** caused by naive fine-tuning of CLIP embeddings  
- âœ… Proposed a **Residual Projection Head (Adapter)** to preserve pre-trained alignment  
- ğŸ“ˆ Improved **Recall@10 from 58.7% â†’ 69.7%** (+11% absolute)  
- ğŸ§  Uses **contrastive learning (InfoNCE)** with efficient feature caching  
- âš™ï¸ Designed with **production-ready retrieval architecture** in mind  
- ğŸ§© Serves as the **retrieval backbone of a RAG-ready system** (generation layer can be added)

---

## ğŸ§  **Problem Motivation**

Traditional keyword-based search struggles to bridge the **vocabulary gap** between:

- User intent: *â€œcute small orange plushieâ€*  
- Product metadata: *â€œToy, Polyester, 5-inchâ€*

Visionâ€“Language Models (VLMs) such as CLIP offer strong zero-shot alignment, but **naive fine-tuning often degrades performance**, especially on small or noisy datasets.

This project explores a key question:

> **How can we adapt CLIP to an e-commerce domain without destroying its pre-trained semantic structure?**

---

## ğŸ—ï¸ **System Architecture**

```text
Text Query
   â†“
CLIP Text Encoder
   â†“
Residual Projection Head
   â†“
Text Embedding
   â†“
Vector Database (FAISS / Milvus)
   â†“
ANN Search
   â†“
Top-K Product Images
